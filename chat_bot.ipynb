{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1SCxxwqwCaGdEKkdwENRumIxUvtPTetqi",
      "authorship_tag": "ABX9TyOXvX3sFJfqYkqvQkpdoztL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Profe-Jose-Burgos/Multi-Bot/blob/main/chat_bot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Entrenamiento\n",
        "\n"
      ],
      "metadata": {
        "id": "C66GMTjHkyhg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HDkf_if19Ewg"
      },
      "outputs": [],
      "source": [
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = open(\"/content/drive/MyDrive/hackathon/tryouts.json\").read()\n",
        "intents = json.loads(data)"
      ],
      "metadata": {
        "id": "DhEhzTaCJnZ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('wordnet')\n",
        "from nltk import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dc9Shao_K6yC",
        "outputId": "271bb9d0-3490-41a2-daa0-a143a660a9ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = []\n",
        "classes = []\n",
        "documents = []\n",
        "ignore_words = ['?','!']"
      ],
      "metadata": {
        "id": "XY-8QfhLLkW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for intent in intents['intents']:\n",
        "    for pattern in intent['patterns']:\n",
        "      x = nltk.word_tokenize(pattern)\n",
        "      words.extend(x)\n",
        "      documents.append((x, intent['tag']))\n",
        "\n",
        "      if intent['tag'] not in classes:\n",
        "        classes.append(intent['tag'])\n",
        "\n",
        "words = [lemmatizer.lemmatize(x.lower()) for x in words if x not in ignore_words]"
      ],
      "metadata": {
        "id": "KrO1aDSULFFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle"
      ],
      "metadata": {
        "id": "AmFiCByNjXvu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pickle.dump(words, open('words.pkl','wb'))\n",
        "pickle.dump(classes, open('classes.pkl','wb'))\n",
        "\n",
        "\n",
        "training = []\n",
        "output_empty = [0] * len(classes)\n",
        "\n",
        "for doc in documents:   \n",
        "    count = []    \n",
        "    pattern_words = doc[0]    \n",
        "    pattern_words = [lemmatizer.lemmatize(word.lower()) for word in pattern_words]\n",
        "\n",
        "    for w in words:\n",
        "        count.append(1) if w in pattern_words else count.append(0)\n",
        "\n",
        "    output_row = list(output_empty)\n",
        "    output_row[classes.index(doc[1])] = 1\n",
        "\n",
        "    training.append([count, output_row])"
      ],
      "metadata": {
        "id": "lu4-czQIjhh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import SGD"
      ],
      "metadata": {
        "id": "az7xhZw9j4mc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training = np.array(training)\n",
        "train_x = list(training[:,0])\n",
        "train_y = list(training[:,1])"
      ],
      "metadata": {
        "id": "u4AxYvCgj4uU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(128, input_shape=(len(train_x[0]),), activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(len(train_y[0]), activation='softmax'))\n",
        "\n",
        "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "\n",
        "#fitting and saving the model\n",
        "hist = model.fit(np.array(train_x), np.array(train_y), epochs=300, batch_size=5, verbose=1)\n",
        "model.save('chatbot_model.h5', hist)"
      ],
      "metadata": {
        "id": "A0Bl9PoskHtJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NrNas8PckNGY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Chat bot"
      ],
      "metadata": {
        "id": "gkOv8Eq0la86"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6_Hpu94lkNI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import load_model"
      ],
      "metadata": {
        "id": "9oE4-e9il9Ch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_model('chatbot_model.h5')\n",
        "intents = json.loads(open('/content/drive/MyDrive/Chat_bot/chat_bot_1.0/intents.json').read())\n",
        "words = pickle.load(open('words.pkl','rb'))\n",
        "classes = pickle.load(open('classes.pkl','rb'))"
      ],
      "metadata": {
        "id": "5IJCafKHkv6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bPyvyGqhkv84"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}